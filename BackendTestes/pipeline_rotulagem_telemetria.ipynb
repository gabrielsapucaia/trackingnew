{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline de Rotulagem Autom√°tica - Telemetria Veicular\n",
        "\n",
        "Este notebook implementa um pipeline completo para rotular automaticamente estados operacionais de caminh√µes baseado em dados de telemetria (1 Hz).\n",
        "\n",
        "**Objetivos:**\n",
        "1. Segmentar estados PARADO vs EM MOVIMENTO\n",
        "2. Rotular estados operacionais: CARREGANDO, BASCULANDO, MOTOR_LIGADO, MOTOR_DESLIGADO, DESCONHECIDO\n",
        "3. Gerar features interpret√°veis e gr√°ficos de valida√ß√£o\n",
        "4. Exportar tabela de eventos rotulados com confian√ßa e evid√™ncias\n",
        "\n",
        "**Arquivo de entrada:** `telemetry_all_data_20251213_171617.csv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C√©lula 1: Configura√ß√£o e Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import os\n",
        "from scipy import signal\n",
        "from scipy.fft import fft, fftfreq\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "try:\n",
        "    import hdbscan\n",
        "    HDBSCAN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    HDBSCAN_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è HDBSCAN n√£o dispon√≠vel. Usando KMeans como fallback.\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes de visualiza√ß√£o\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Configura√ß√µes do pipeline\n",
        "CONFIG = {\n",
        "    'V_STOP': 0.5,              # km/h threshold para considerar parado\n",
        "    'MIN_STOP_SEC': 10,          # dura√ß√£o m√≠nima de parada (segundos)\n",
        "    'GAP_SEC': 3,                # toler√¢ncia a gaps em segmentos (segundos)\n",
        "    'PEAK_HEIGHT': 0.5,          # altura m√≠nima de pico (m/s¬≤)\n",
        "    'PEAK_DISTANCE': 5,          # dist√¢ncia m√≠nima entre picos (segundos)\n",
        "    'TH_CONF': 0.5,              # threshold de confian√ßa m√≠nimo\n",
        "    'MAX_SHORT_MOVE_SEC': 30,    # dura√ß√£o m√°xima de andadinha curta\n",
        "    'MAX_SHORT_MOVE_SPEED': 5,   # velocidade m√°xima de andadinha curta (km/h)\n",
        "    'WINDOW_SEC': 10,            # janela para features deslizantes\n",
        "}\n",
        "\n",
        "# Caminhos\n",
        "CSV_PATH = '/Users/sapucaia/tracking/BackendTestes/telemetry_all_data_20251213_171617.csv'\n",
        "OUTPUT_DIR = Path('/Users/sapucaia/tracking/BackendTestes/labeled_segments')\n",
        "PLOTS_DIR = Path('/Users/sapucaia/tracking/BackendTestes/plots')\n",
        "\n",
        "# Criar diret√≥rios\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "PLOTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Configura√ß√£o conclu√≠da\")\n",
        "print(f\"üìÅ Diret√≥rio de sa√≠da: {OUTPUT_DIR}\")\n",
        "print(f\"üìÅ Diret√≥rio de gr√°ficos: {PLOTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Using cached pandas-2.3.3-cp314-cp314-macosx_11_0_arm64.whl.metadata (91 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.3.5-cp314-cp314-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.8-cp314-cp314-macosx_11_0_arm64.whl.metadata (52 kB)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.16.3-cp314-cp314-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
            "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
            "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
            "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
            "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m More information is available at\n",
            "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[31mERROR: Failed to build 'sklearn' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas numpy matplotlib seaborn scipy sklearn hdbscan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C√©lula 2: Data Discovery\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar CSV\n",
        "print(\"‚è≥ Carregando CSV...\")\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "\n",
        "# Converter coluna time para datetime\n",
        "df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "print(f\"‚úÖ Dados carregados: {len(df):,} registros, {len(df.columns)} colunas\")\n",
        "print(f\"üìÖ Per√≠odo: {df['time'].min()} at√© {df['time'].max()}\")\n",
        "print(f\"‚è±Ô∏è Dura√ß√£o total: {(df['time'].max() - df['time'].min()).total_seconds() / 3600:.2f} horas\")\n",
        "print(f\"üöõ Devices √∫nicos: {df['device_id'].nunique()}\")\n",
        "print(f\"üìä Taxa de amostragem m√©dia: {len(df) / ((df['time'].max() - df['time'].min()).total_seconds() / 3600):.2f} Hz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerar Data Dictionary\n",
        "print(\"üìö Gerando Data Dictionary...\\n\")\n",
        "\n",
        "data_dict = []\n",
        "for col in df.columns:\n",
        "    dtype = str(df[col].dtype)\n",
        "    missing_pct = (df[col].isna().sum() / len(df)) * 100\n",
        "    \n",
        "    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "        col_min = df[col].min()\n",
        "        col_max = df[col].max()\n",
        "        col_mean = df[col].mean()\n",
        "        col_std = df[col].std()\n",
        "        range_info = f\"[{col_min:.4f}, {col_max:.4f}], mean={col_mean:.4f}, std={col_std:.4f}\"\n",
        "    else:\n",
        "        unique_count = df[col].nunique()\n",
        "        range_info = f\"{unique_count} valores √∫nicos\"\n",
        "    \n",
        "    data_dict.append({\n",
        "        'Coluna': col,\n",
        "        'Tipo': dtype,\n",
        "        'Missing (%)': f\"{missing_pct:.2f}\",\n",
        "        'Range/Stats': range_info\n",
        "    })\n",
        "\n",
        "df_dict = pd.DataFrame(data_dict)\n",
        "print(df_dict.to_string(index=False))\n",
        "\n",
        "# Identificar colunas-chave\n",
        "print(\"\\nüîë Colunas-chave identificadas:\")\n",
        "key_cols = {\n",
        "    'timestamp': 'time',\n",
        "    'device_id': 'device_id',\n",
        "    'speed': 'speed_kmh',\n",
        "    'accel_linear': 'linear_accel_magnitude',\n",
        "    'pitch': 'pitch',\n",
        "    'roll': 'roll',\n",
        "    'battery_status': 'battery_status',\n",
        "    'battery_voltage': 'battery_voltage',\n",
        "    'motion_stationary': 'motion_stationary_detect'\n",
        "}\n",
        "\n",
        "for key, col in key_cols.items():\n",
        "    if col in df.columns:\n",
        "        print(f\"  ‚úÖ {key}: {col}\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå {key}: N√ÉO ENCONTRADO\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar qualidade dos sinais principais\n",
        "print(\"üìä Qualidade dos Sinais Principais:\\n\")\n",
        "\n",
        "for col in ['speed_kmh', 'linear_accel_magnitude', 'pitch', 'roll', 'battery_voltage']:\n",
        "    if col in df.columns:\n",
        "        missing = df[col].isna().sum()\n",
        "        pct_missing = (missing / len(df)) * 100\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            valid = df[col].notna()\n",
        "            if valid.sum() > 0:\n",
        "                print(f\"{col}:\")\n",
        "                print(f\"  Missing: {missing} ({pct_missing:.2f}%)\")\n",
        "                print(f\"  Range: [{df[col].min():.4f}, {df[col].max():.4f}]\")\n",
        "                print(f\"  Mean: {df[col].mean():.4f}, Std: {df[col].std():.4f}\")\n",
        "                print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C√©lula 3: Limpeza e Qualidade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ordenar por time e device_id\n",
        "df = df.sort_values(['device_id', 'time']).reset_index(drop=True)\n",
        "\n",
        "# Verificar duplicatas\n",
        "duplicates = df.duplicated(subset=['device_id', 'time']).sum()\n",
        "print(f\"üîç Duplicatas encontradas: {duplicates}\")\n",
        "if duplicates > 0:\n",
        "    df = df.drop_duplicates(subset=['device_id', 'time'], keep='first')\n",
        "    print(f\"‚úÖ Duplicatas removidas. Registros restantes: {len(df):,}\")\n",
        "\n",
        "# Verificar gaps de amostragem\n",
        "df['time_diff'] = df.groupby('device_id')['time'].diff().dt.total_seconds()\n",
        "gap_stats = df['time_diff'].describe()\n",
        "print(f\"\\nüìà Estat√≠sticas de intervalo entre amostras:\")\n",
        "print(f\"  M√©dia: {gap_stats['mean']:.2f}s\")\n",
        "print(f\"  Mediana: {gap_stats['50%']:.2f}s\")\n",
        "print(f\"  P95: {gap_stats['95%']:.2f}s\")\n",
        "print(f\"  Max: {gap_stats['max']:.2f}s\")\n",
        "\n",
        "# Identificar outliers grosseiros em speed_kmh\n",
        "if 'speed_kmh' in df.columns:\n",
        "    # Remover valores negativos ou extremamente altos (>200 km/h para caminh√£o)\n",
        "    outliers_speed = (df['speed_kmh'] < 0) | (df['speed_kmh'] > 200)\n",
        "    print(f\"\\nüö® Outliers em speed_kmh: {outliers_speed.sum()}\")\n",
        "    if outliers_speed.sum() > 0:\n",
        "        df.loc[outliers_speed, 'speed_kmh'] = np.nan\n",
        "        print(f\"‚úÖ Outliers substitu√≠dos por NaN\")\n",
        "\n",
        "# Identificar outliers em linear_accel_magnitude\n",
        "if 'linear_accel_magnitude' in df.columns:\n",
        "    # Valores muito altos (>50 m/s¬≤ s√£o suspeitos para caminh√£o)\n",
        "    outliers_accel = df['linear_accel_magnitude'].abs() > 50\n",
        "    print(f\"üö® Outliers em linear_accel_magnitude: {outliers_accel.sum()}\")\n",
        "    if outliers_accel.sum() > 0:\n",
        "        df.loc[outliers_accel, 'linear_accel_magnitude'] = np.nan\n",
        "        print(f\"‚úÖ Outliers substitu√≠dos por NaN\")\n",
        "\n",
        "print(f\"\\n‚úÖ Limpeza conclu√≠da. Registros finais: {len(df):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gr√°ficos de qualidade\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# 1. Missing por coluna (top 20)\n",
        "missing_counts = df.isnull().sum().sort_values(ascending=False).head(20)\n",
        "axes[0, 0].barh(range(len(missing_counts)), missing_counts.values)\n",
        "axes[0, 0].set_yticks(range(len(missing_counts)))\n",
        "axes[0, 0].set_yticklabels(missing_counts.index)\n",
        "axes[0, 0].set_xlabel('Quantidade de Missing')\n",
        "axes[0, 0].set_title('Top 20 Colunas com Missing Values')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Histograma de speed_kmh\n",
        "if 'speed_kmh' in df.columns:\n",
        "    valid_speed = df['speed_kmh'].dropna()\n",
        "    axes[0, 1].hist(valid_speed, bins=100, edgecolor='black', alpha=0.7)\n",
        "    axes[0, 1].axvline(CONFIG['V_STOP'], color='r', linestyle='--', label=f\"V_STOP={CONFIG['V_STOP']} km/h\")\n",
        "    axes[0, 1].set_xlabel('Velocidade (km/h)')\n",
        "    axes[0, 1].set_ylabel('Frequ√™ncia')\n",
        "    axes[0, 1].set_title('Distribui√ß√£o de Velocidade')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Histograma de linear_accel_magnitude\n",
        "if 'linear_accel_magnitude' in df.columns:\n",
        "    valid_accel = df['linear_accel_magnitude'].dropna()\n",
        "    axes[1, 0].hist(valid_accel, bins=100, edgecolor='black', alpha=0.7)\n",
        "    axes[1, 0].axvline(CONFIG['PEAK_HEIGHT'], color='r', linestyle='--', label=f\"PEAK_HEIGHT={CONFIG['PEAK_HEIGHT']} m/s¬≤\")\n",
        "    axes[1, 0].set_xlabel('Acelera√ß√£o Linear Magnitude (m/s¬≤)')\n",
        "    axes[1, 0].set_ylabel('Frequ√™ncia')\n",
        "    axes[1, 0].set_title('Distribui√ß√£o de Acelera√ß√£o Linear')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Timeline de amostragem (primeiras 1000 amostras)\n",
        "sample_df = df.head(1000)\n",
        "axes[1, 1].plot(sample_df['time'], range(len(sample_df)), marker='.', markersize=1)\n",
        "axes[1, 1].set_xlabel('Tempo')\n",
        "axes[1, 1].set_ylabel('√çndice da Amostra')\n",
        "axes[1, 1].set_title('Timeline de Amostragem (primeiras 1000)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / '01_data_quality.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"‚úÖ Gr√°fico salvo: {PLOTS_DIR / '01_data_quality.png'}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C√©lula 4: Segmenta√ß√£o Base (PARADO vs MOVIMENTO)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_stop_segments(df, speed_col='speed_kmh', v_stop=0.5, min_stop_sec=10, gap_sec=3):\n",
        "    \"\"\"\n",
        "    Identifica segmentos cont√≠nuos onde o ve√≠culo est√° parado.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame com coluna time e speed_col\n",
        "        speed_col: nome da coluna de velocidade\n",
        "        v_stop: threshold de velocidade para considerar parado (km/h)\n",
        "        min_stop_sec: dura√ß√£o m√≠nima do segmento (segundos)\n",
        "        gap_sec: toler√¢ncia a gaps dentro do segmento (segundos)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame com colunas: device_id, t_start, t_end, duration_s, is_moving\n",
        "    \"\"\"\n",
        "    segments = []\n",
        "    \n",
        "    for device_id in df['device_id'].unique():\n",
        "        device_df = df[df['device_id'] == device_id].copy()\n",
        "        device_df = device_df.sort_values('time').reset_index(drop=True)\n",
        "        \n",
        "        # Marcar pontos parados\n",
        "        device_df['is_stopped'] = device_df[speed_col] <= v_stop\n",
        "        \n",
        "        # Criar grupos cont√≠nuos de parada\n",
        "        device_df['group'] = (device_df['is_stopped'] != device_df['is_stopped'].shift()).cumsum()\n",
        "        \n",
        "        for group_id in device_df['group'].unique():\n",
        "            group_data = device_df[device_df['group'] == group_id]\n",
        "            \n",
        "            if group_data['is_stopped'].iloc[0]:  # Apenas grupos de parada\n",
        "                t_start = group_data['time'].iloc[0]\n",
        "                t_end = group_data['time'].iloc[-1]\n",
        "                duration_s = (t_end - t_start).total_seconds()\n",
        "                \n",
        "                # Verificar gaps dentro do segmento\n",
        "                time_diffs = group_data['time'].diff().dt.total_seconds()\n",
        "                max_gap = time_diffs.max() if len(time_diffs) > 1 else 0\n",
        "                \n",
        "                # Aplicar filtros\n",
        "                if duration_s >= min_stop_sec and max_gap <= gap_sec:\n",
        "                    segments.append({\n",
        "                        'device_id': device_id,\n",
        "                        't_start': t_start,\n",
        "                        't_end': t_end,\n",
        "                        'duration_s': duration_s,\n",
        "                        'is_moving': False\n",
        "                    })\n",
        "        \n",
        "        # Criar segmentos de movimento (complemento)\n",
        "        device_df['is_moving'] = device_df[speed_col] > v_stop\n",
        "        device_df['move_group'] = (device_df['is_moving'] != device_df['is_moving'].shift()).cumsum()\n",
        "        \n",
        "        for group_id in device_df['move_group'].unique():\n",
        "            group_data = device_df[device_df['move_group'] == group_id]\n",
        "            \n",
        "            if group_data['is_moving'].iloc[0]:\n",
        "                t_start = group_data['time'].iloc[0]\n",
        "                t_end = group_data['time'].iloc[-1]\n",
        "                duration_s = (t_end - t_start).total_seconds()\n",
        "                \n",
        "                if duration_s >= min_stop_sec:  # Movimento tamb√©m precisa de dura√ß√£o m√≠nima\n",
        "                    segments.append({\n",
        "                        'device_id': device_id,\n",
        "                        't_start': t_start,\n",
        "                        't_end': t_end,\n",
        "                        'duration_s': duration_s,\n",
        "                        'is_moving': True\n",
        "                    })\n",
        "    \n",
        "    segments_df = pd.DataFrame(segments)\n",
        "    if len(segments_df) > 0:\n",
        "        segments_df = segments_df.sort_values(['device_id', 't_start']).reset_index(drop=True)\n",
        "    \n",
        "    return segments_df\n",
        "\n",
        "# Aplicar segmenta√ß√£o\n",
        "print(\"‚è≥ Segmentando dados...\")\n",
        "segments_df = find_stop_segments(\n",
        "    df,\n",
        "    speed_col='speed_kmh',\n",
        "    v_stop=CONFIG['V_STOP'],\n",
        "    min_stop_sec=CONFIG['MIN_STOP_SEC'],\n",
        "    gap_sec=CONFIG['GAP_SEC']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Segmenta√ß√£o conclu√≠da:\")\n",
        "print(f\"  Total de segmentos: {len(segments_df)}\")\n",
        "print(f\"  Segmentos PARADO: {len(segments_df[segments_df['is_moving'] == False])}\")\n",
        "print(f\"  Segmentos MOVIMENTO: {len(segments_df[segments_df['is_moving'] == True])}\")\n",
        "print(f\"\\nüìä Estat√≠sticas de dura√ß√£o:\")\n",
        "print(segments_df.groupby('is_moving')['duration_s'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar segmenta√ß√£o\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "# Timeline com segmentos coloridos\n",
        "for device_id in df['device_id'].unique():\n",
        "    device_df = df[df['device_id'] == device_id].copy()\n",
        "    device_segments = segments_df[segments_df['device_id'] == device_id]\n",
        "    \n",
        "    # Plot velocidade\n",
        "    axes[0].plot(device_df['time'], device_df['speed_kmh'], alpha=0.3, label=f'{device_id} - Velocidade', color='gray')\n",
        "    \n",
        "    # Colorir segmentos de parada\n",
        "    for _, seg in device_segments[device_segments['is_moving'] == False].iterrows():\n",
        "        axes[0].axvspan(seg['t_start'], seg['t_end'], alpha=0.3, color='red', label='PARADO' if seg.name == 0 else '')\n",
        "    \n",
        "    # Colorir segmentos de movimento\n",
        "    for _, seg in device_segments[device_segments['is_moving'] == True].iterrows():\n",
        "        axes[0].axvspan(seg['t_start'], seg['t_end'], alpha=0.3, color='green', label='MOVIMENTO' if seg.name == 0 else '')\n",
        "\n",
        "axes[0].set_xlabel('Tempo')\n",
        "axes[0].set_ylabel('Velocidade (km/h)')\n",
        "axes[0].set_title('Timeline de Segmenta√ß√£o: PARADO vs MOVIMENTO')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axhline(CONFIG['V_STOP'], color='r', linestyle='--', linewidth=2, label=f\"V_STOP={CONFIG['V_STOP']} km/h\")\n",
        "\n",
        "# Distribui√ß√£o de dura√ß√µes\n",
        "axes[1].hist(segments_df[segments_df['is_moving'] == False]['duration_s'] / 60, bins=50, \n",
        "             alpha=0.7, label='PARADO', color='red', edgecolor='black')\n",
        "axes[1].hist(segments_df[segments_df['is_moving'] == True]['duration_s'] / 60, bins=50, \n",
        "             alpha=0.7, label='MOVIMENTO', color='green', edgecolor='black')\n",
        "axes[1].set_xlabel('Dura√ß√£o (minutos)')\n",
        "axes[1].set_ylabel('Frequ√™ncia')\n",
        "axes[1].set_title('Distribui√ß√£o de Dura√ß√£o dos Segmentos')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / '02_segmentation.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"‚úÖ Gr√°fico salvo: {PLOTS_DIR / '02_segmentation.png'}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C√©lula 5: Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features(seg_data, window_sec=10):\n",
        "    \"\"\"\n",
        "    Extrai features de um segmento de dados.\n",
        "    \n",
        "    Args:\n",
        "        seg_data: DataFrame com dados do segmento (deve ter time, linear_accel_magnitude, pitch, roll, etc.)\n",
        "        window_sec: tamanho da janela para features deslizantes\n",
        "    \n",
        "    Returns:\n",
        "        dict com features extra√≠das\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    # Features de acelera√ß√£o linear\n",
        "    if 'linear_accel_magnitude' in seg_data.columns:\n",
        "        accel = seg_data['linear_accel_magnitude'].dropna()\n",
        "        if len(accel) > 0:\n",
        "            features['accel_mean'] = accel.mean()\n",
        "            features['accel_std'] = accel.std()\n",
        "            features['accel_rms'] = np.sqrt((accel**2).mean())\n",
        "            features['accel_p50'] = accel.median()\n",
        "            features['accel_p95'] = accel.quantile(0.95)\n",
        "            features['accel_p99'] = accel.quantile(0.99)\n",
        "            features['accel_iqr'] = accel.quantile(0.75) - accel.quantile(0.25)\n",
        "            features['accel_energy'] = (accel**2).sum()\n",
        "            features['accel_max'] = accel.max()\n",
        "            features['accel_min'] = accel.min()\n",
        "            \n",
        "            # Detec√ß√£o de picos\n",
        "            peaks, properties = signal.find_peaks(\n",
        "                accel.values,\n",
        "                height=CONFIG['PEAK_HEIGHT'],\n",
        "                distance=int(CONFIG['PEAK_DISTANCE'])\n",
        "            )\n",
        "            features['peak_count'] = len(peaks)\n",
        "            if len(peaks) > 0:\n",
        "                peak_heights = accel.iloc[peaks].values\n",
        "                features['peak_height_mean'] = peak_heights.mean()\n",
        "                features['peak_height_max'] = peak_heights.max()\n",
        "                \n",
        "                # Intervalos entre picos\n",
        "                if len(peaks) > 1:\n",
        "                    peak_intervals = np.diff(peaks)\n",
        "                    features['peak_interval_mean'] = peak_intervals.mean()\n",
        "                    features['peak_interval_std'] = peak_intervals.std()\n",
        "                else:\n",
        "                    features['peak_interval_mean'] = np.nan\n",
        "                    features['peak_interval_std'] = np.nan\n",
        "            else:\n",
        "                features['peak_height_mean'] = 0\n",
        "                features['peak_height_max'] = 0\n",
        "                features['peak_interval_mean'] = np.nan\n",
        "                features['peak_interval_std'] = np.nan\n",
        "            \n",
        "            # An√°lise espectral (FFT)\n",
        "            if len(accel) >= 10:\n",
        "                fft_vals = np.abs(fft(accel.values))\n",
        "                freqs = fftfreq(len(accel), 1.0)  # 1 Hz sampling\n",
        "                # Energia em diferentes bandas\n",
        "                low_freq_energy = np.sum(fft_vals[freqs < 0.1])  # < 0.1 Hz\n",
        "                mid_freq_energy = np.sum(fft_vals[(freqs >= 0.1) & (freqs < 0.5)])  # 0.1-0.5 Hz\n",
        "                high_freq_energy = np.sum(fft_vals[freqs >= 0.5])  # >= 0.5 Hz\n",
        "                features['spectral_low_energy'] = low_freq_energy\n",
        "                features['spectral_mid_energy'] = mid_freq_energy\n",
        "                features['spectral_high_energy'] = high_freq_energy\n",
        "            else:\n",
        "                features['spectral_low_energy'] = np.nan\n",
        "                features['spectral_mid_energy'] = np.nan\n",
        "                features['spectral_high_energy'] = np.nan\n",
        "        else:\n",
        "            # Preencher com NaN se n√£o houver dados\n",
        "            for key in ['accel_mean', 'accel_std', 'accel_rms', 'accel_p50', 'accel_p95', 'accel_p99',\n",
        "                       'accel_iqr', 'accel_energy', 'accel_max', 'accel_min', 'peak_count',\n",
        "                       'peak_height_mean', 'peak_height_max', 'peak_interval_mean', 'peak_interval_std',\n",
        "                       'spectral_low_energy', 'spectral_mid_energy', 'spectral_high_energy']:\n",
        "                features[key] = np.nan\n",
        "    \n",
        "    # Features de orienta√ß√£o (pitch e roll)\n",
        "    if 'pitch' in seg_data.columns:\n",
        "        pitch = seg_data['pitch'].dropna()\n",
        "        if len(pitch) > 0:\n",
        "            features['pitch_mean'] = pitch.mean()\n",
        "            features['pitch_std'] = pitch.std()\n",
        "            features['pitch_delta'] = pitch.max() - pitch.min()\n",
        "            features['pitch_range'] = pitch.max() - pitch.min()\n",
        "        else:\n",
        "            features['pitch_mean'] = np.nan\n",
        "            features['pitch_std'] = np.nan\n",
        "            features['pitch_delta'] = np.nan\n",
        "            features['pitch_range'] = np.nan\n",
        "    \n",
        "    if 'roll' in seg_data.columns:\n",
        "        roll = seg_data['roll'].dropna()\n",
        "        if len(roll) > 0:\n",
        "            features['roll_mean'] = roll.mean()\n",
        "            features['roll_std'] = roll.std()\n",
        "            features['roll_delta'] = roll.max() - roll.min()\n",
        "            features['roll_range'] = roll.max() - roll.min()\n",
        "        else:\n",
        "            features['roll_mean'] = np.nan\n",
        "            features['roll_std'] = np.nan\n",
        "            features['roll_delta'] = np.nan\n",
        "            features['roll_range'] = np.nan\n",
        "    \n",
        "    # Features de bateria (proxy para motor ligado/desligado)\n",
        "    if 'battery_voltage' in seg_data.columns:\n",
        "        voltage = seg_data['battery_voltage'].dropna()\n",
        "        if len(voltage) > 0:\n",
        "            features['voltage_mean'] = voltage.mean()\n",
        "            features['voltage_std'] = voltage.std()\n",
        "        else:\n",
        "            features['voltage_mean'] = np.nan\n",
        "            features['voltage_std'] = np.nan\n",
        "    \n",
        "    if 'battery_status' in seg_data.columns:\n",
        "        status = seg_data['battery_status'].dropna()\n",
        "        if len(status) > 0:\n",
        "            # Contar ocorr√™ncias de CHARGING (motor ligado)\n",
        "            features['battery_charging_pct'] = (status == 'CHARGING').sum() / len(status) if len(status) > 0 else 0\n",
        "        else:\n",
        "            features['battery_charging_pct'] = np.nan\n",
        "    \n",
        "    return features\n",
        "\n",
        "# Extrair features para cada segmento PARADO\n",
        "print(\"‚è≥ Extraindo features dos segmentos PARADO...\")\n",
        "stop_segments = segments_df[segments_df['is_moving'] == False].copy()\n",
        "\n",
        "features_list = []\n",
        "for idx, seg in stop_segments.iterrows():\n",
        "    # Obter dados do segmento\n",
        "    seg_data = df[\n",
        "        (df['device_id'] == seg['device_id']) &\n",
        "        (df['time'] >= seg['t_start']) &\n",
        "        (df['time'] <= seg['t_end'])\n",
        "    ].copy()\n",
        "    \n",
        "    if len(seg_data) > 0:\n",
        "        feat = extract_features(seg_data, window_sec=CONFIG['WINDOW_SEC'])\n",
        "        feat['segment_idx'] = idx\n",
        "        features_list.append(feat)\n",
        "\n",
        "features_df = pd.DataFrame(features_list)\n",
        "print(f\"‚úÖ Features extra√≠das para {len(features_df)} segmentos\")\n",
        "print(f\"\\nüìä Features dispon√≠veis: {list(features_df.columns)}\")\n",
        "print(f\"\\nüìà Estat√≠sticas das features principais:\")\n",
        "print(features_df[['accel_mean', 'accel_std', 'peak_count', 'pitch_delta', 'voltage_mean']].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_stop_segment(features, th_conf=0.5):\n",
        "    \"\"\"\n",
        "    Classifica um segmento parado usando regras de weak supervision.\n",
        "    \n",
        "    Args:\n",
        "        features: dict com features do segmento\n",
        "        th_conf: threshold m√≠nimo de confian√ßa\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (label, confidence, evidence)\n",
        "    \"\"\"\n",
        "    label = 'DESCONHECIDO'\n",
        "    confidence = 0.0\n",
        "    evidence_parts = []\n",
        "    \n",
        "    # Regra 1: CARREGANDO\n",
        "    # M√∫ltiplos picos intermitentes (alta variabilidade de intervalos)\n",
        "    peak_count = features.get('peak_count', 0)\n",
        "    peak_interval_std = features.get('peak_interval_std', np.nan)\n",
        "    accel_std = features.get('accel_std', 0)\n",
        "    \n",
        "    if peak_count >= 5 and not np.isnan(peak_interval_std) and peak_interval_std > 3:\n",
        "        conf_carregando = min(0.9, 0.5 + (peak_count / 20) * 0.4)\n",
        "        if conf_carregando > confidence:\n",
        "            label = 'CARREGANDO'\n",
        "            confidence = conf_carregando\n",
        "            evidence_parts.append(f\"peak_count={peak_count}, interval_std={peak_interval_std:.2f}\")\n",
        "    \n",
        "    # Regra 2: BASCULANDO\n",
        "    # Mudan√ßa significativa de pitch OU padr√£o cont√≠nuo de acelera√ß√£o\n",
        "    pitch_delta = features.get('pitch_delta', 0)\n",
        "    accel_rms = features.get('accel_rms', 0)\n",
        "    accel_mean = features.get('accel_mean', 0)\n",
        "    \n",
        "    if not np.isnan(pitch_delta) and pitch_delta > 10:  # >10 graus de mudan√ßa\n",
        "        conf_basculando = min(0.8, 0.5 + (pitch_delta / 30) * 0.3)\n",
        "        if conf_basculando > confidence:\n",
        "            label = 'BASCULANDO'\n",
        "            confidence = conf_basculando\n",
        "            evidence_parts.append(f\"pitch_delta={pitch_delta:.2f}deg\")\n",
        "    elif accel_rms > 1.5 and accel_mean > 0.5:  # Padr√£o cont√≠nuo de vibra√ß√£o\n",
        "        conf_basculando = min(0.7, 0.4 + (accel_rms / 3) * 0.3)\n",
        "        if conf_basculando > confidence:\n",
        "            label = 'BASCULANDO'\n",
        "            confidence = conf_basculando\n",
        "            evidence_parts.append(f\"accel_rms={accel_rms:.2f}, accel_mean={accel_mean:.2f}\")\n",
        "    \n",
        "    # Regra 3: MOTOR_DESLIGADO\n",
        "    # Baixa voltagem E baixa variabilidade de acelera√ß√£o\n",
        "    voltage_mean = features.get('voltage_mean', np.nan)\n",
        "    battery_charging_pct = features.get('battery_charging_pct', 0)\n",
        "    \n",
        "    if not np.isnan(voltage_mean) and voltage_mean < 4000:  # Threshold emp√≠rico\n",
        "        if accel_std < 0.3 and accel_rms < 0.5:\n",
        "            conf_motor_off = min(0.95, 0.7 + (1 - battery_charging_pct) * 0.25)\n",
        "            if conf_motor_off > confidence:\n",
        "                label = 'MOTOR_DESLIGADO'\n",
        "                confidence = conf_motor_off\n",
        "                evidence_parts.append(f\"voltage={voltage_mean:.0f}mV, accel_std={accel_std:.2f}\")\n",
        "    \n",
        "    # Regra 4: MOTOR_LIGADO_IDLE\n",
        "    # Voltagem OK E acelera√ß√£o moderada cont√≠nua (sem picos intermitentes)\n",
        "    if not np.isnan(voltage_mean) and voltage_mean >= 4000:\n",
        "        if accel_std > 0.3 and accel_std < 1.5 and peak_count < 3:\n",
        "            conf_motor_on = min(0.7, 0.5 + (accel_std / 2) * 0.2)\n",
        "            if conf_motor_on > confidence:\n",
        "                label = 'MOTOR_LIGADO_IDLE'\n",
        "                confidence = conf_motor_on\n",
        "                evidence_parts.append(f\"voltage={voltage_mean:.0f}mV, accel_std={accel_std:.2f}, peaks={peak_count}\")\n",
        "    \n",
        "    # Se nenhuma regra atingiu threshold, manter DESCONHECIDO\n",
        "    if confidence < th_conf:\n",
        "        label = 'DESCONHECIDO'\n",
        "        confidence = confidence  # Manter confian√ßa calculada mesmo que baixa\n",
        "        evidence_parts.append(\"nenhuma regra atingiu threshold\")\n",
        "    \n",
        "    evidence = \" | \".join(evidence_parts) if evidence_parts else \"sem evid√™ncia\"\n",
        "    \n",
        "    return label, confidence, evidence\n",
        "\n",
        "# Aplicar rotulagem\n",
        "print(\"‚è≥ Aplicando rotulagem aos segmentos PARADO...\")\n",
        "\n",
        "labeled_segments = []\n",
        "for idx, seg in stop_segments.iterrows():\n",
        "    seg_features = features_df[features_df['segment_idx'] == idx]\n",
        "    \n",
        "    if len(seg_features) > 0:\n",
        "        feat_dict = seg_features.iloc[0].to_dict()\n",
        "        label, confidence, evidence = classify_stop_segment(feat_dict, th_conf=CONFIG['TH_CONF'])\n",
        "        \n",
        "        labeled_segments.append({\n",
        "            'device_id': seg['device_id'],\n",
        "            't_start': seg['t_start'],\n",
        "            't_end': seg['t_end'],\n",
        "            'duration_s': seg['duration_s'],\n",
        "            'is_moving': False,\n",
        "            'label_operacional': label,\n",
        "            'confidence': confidence,\n",
        "            'evidence': evidence,\n",
        "            **feat_dict\n",
        "        })\n",
        "\n",
        "# Adicionar segmentos de movimento\n",
        "for idx, seg in segments_df[segments_df['is_moving'] == True].iterrows():\n",
        "    labeled_segments.append({\n",
        "        'device_id': seg['device_id'],\n",
        "        't_start': seg['t_start'],\n",
        "        't_end': seg['t_end'],\n",
        "        'duration_s': seg['duration_s'],\n",
        "        'is_moving': True,\n",
        "        'label_operacional': 'EM_MOVIMENTO',\n",
        "        'confidence': 1.0,\n",
        "        'evidence': 'velocidade > V_STOP'\n",
        "    })\n",
        "\n",
        "labeled_df = pd.DataFrame(labeled_segments)\n",
        "\n",
        "print(f\"‚úÖ Rotulagem conclu√≠da:\")\n",
        "print(f\"\\nüìä Distribui√ß√£o de r√≥tulos:\")\n",
        "print(labeled_df['label_operacional'].value_counts())\n",
        "print(f\"\\nüìà Confian√ßa m√©dia por r√≥tulo:\")\n",
        "print(labeled_df.groupby('label_operacional')['confidence'].mean().sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_basculamento_episodes(labeled_df, max_short_move_sec=30, max_short_move_speed=5):\n",
        "    \"\"\"\n",
        "    Mescla segmentos de basculamento com andadinhas curtas adjacentes.\n",
        "    \n",
        "    Args:\n",
        "        labeled_df: DataFrame com segmentos rotulados\n",
        "        max_short_move_sec: dura√ß√£o m√°xima da andadinha\n",
        "        max_short_move_speed: velocidade m√°xima da andadinha\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame com epis√≥dios mesclados\n",
        "    \"\"\"\n",
        "    merged_segments = []\n",
        "    i = 0\n",
        "    \n",
        "    while i < len(labeled_df):\n",
        "        seg = labeled_df.iloc[i]\n",
        "        \n",
        "        # Se √© um segmento de basculamento\n",
        "        if seg['label_operacional'] == 'BASCULANDO' and not seg['is_moving']:\n",
        "            episode_segments = [seg]\n",
        "            episode_start = seg['t_start']\n",
        "            episode_end = seg['t_end']\n",
        "            episode_device = seg['device_id']\n",
        "            \n",
        "            # Verificar pr√≥ximo segmento (movimento curto)\n",
        "            if i + 1 < len(labeled_df):\n",
        "                next_seg = labeled_df.iloc[i + 1]\n",
        "                \n",
        "                # Se √© movimento curto adjacente\n",
        "                if (next_seg['device_id'] == episode_device and\n",
        "                    next_seg['is_moving'] and\n",
        "                    next_seg['duration_s'] <= max_short_move_sec):\n",
        "                    \n",
        "                    # Verificar velocidade m√©dia do movimento\n",
        "                    move_data = df[\n",
        "                        (df['device_id'] == next_seg['device_id']) &\n",
        "                        (df['time'] >= next_seg['t_start']) &\n",
        "                        (df['time'] <= next_seg['t_end'])\n",
        "                    ]\n",
        "                    \n",
        "                    if len(move_data) > 0 and move_data['speed_kmh'].mean() <= max_short_move_speed:\n",
        "                        episode_segments.append(next_seg)\n",
        "                        episode_end = next_seg['t_end']\n",
        "                        i += 1  # Pular o segmento de movimento\n",
        "                        \n",
        "                        # Verificar se h√° outro segmento parado ap√≥s (fim do basculamento)\n",
        "                        if i + 1 < len(labeled_df):\n",
        "                            after_seg = labeled_df.iloc[i + 1]\n",
        "                            if (after_seg['device_id'] == episode_device and\n",
        "                                not after_seg['is_moving'] and\n",
        "                                (after_seg['t_start'] - episode_end).total_seconds() < 10):\n",
        "                                episode_segments.append(after_seg)\n",
        "                                episode_end = after_seg['t_end']\n",
        "                                i += 1\n",
        "            \n",
        "            # Criar epis√≥dio mesclado\n",
        "            episode_duration = (episode_end - episode_start).total_seconds()\n",
        "            merged_segments.append({\n",
        "                'device_id': episode_device,\n",
        "                't_start': episode_start,\n",
        "                't_end': episode_end,\n",
        "                'duration_s': episode_duration,\n",
        "                'is_moving': False,\n",
        "                'label_operacional': 'BASCULANDO',\n",
        "                'confidence': seg['confidence'],\n",
        "                'evidence': f\"{seg['evidence']} + andadinha curta mesclada\",\n",
        "                'has_short_move': len(episode_segments) > 1\n",
        "            })\n",
        "        else:\n",
        "            # Manter segmento original\n",
        "            merged_segments.append(seg.to_dict())\n",
        "        \n",
        "        i += 1\n",
        "    \n",
        "    return pd.DataFrame(merged_segments)\n",
        "\n",
        "# Aplicar merge\n",
        "print(\"‚è≥ Mesclando epis√≥dios de basculamento com andadinhas...\")\n",
        "merged_df = merge_basculamento_episodes(\n",
        "    labeled_df,\n",
        "    max_short_move_sec=CONFIG['MAX_SHORT_MOVE_SEC'],\n",
        "    max_short_move_speed=CONFIG['MAX_SHORT_MOVE_SPEED']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Merge conclu√≠do:\")\n",
        "print(f\"  Segmentos antes: {len(labeled_df)}\")\n",
        "print(f\"  Segmentos depois: {len(merged_df)}\")\n",
        "print(f\"  Epis√≥dios mesclados: {len(merged_df[merged_df.get('has_short_move', False) == True])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C√©lula 8: Clustering Explorat√≥rio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar dados para clustering (apenas segmentos PARADO)\n",
        "stop_labeled = merged_df[merged_df['is_moving'] == False].copy()\n",
        "\n",
        "# Selecionar features num√©ricas relevantes\n",
        "feature_cols = [\n",
        "    'accel_mean', 'accel_std', 'accel_rms', 'accel_p95', 'accel_p99',\n",
        "    'peak_count', 'peak_height_mean', 'peak_interval_mean', 'peak_interval_std',\n",
        "    'pitch_delta', 'roll_delta', 'voltage_mean', 'spectral_low_energy',\n",
        "    'spectral_mid_energy', 'spectral_high_energy'\n",
        "]\n",
        "\n",
        "# Filtrar colunas que existem\n",
        "available_features = [col for col in feature_cols if col in stop_labeled.columns]\n",
        "print(f\"üìä Features dispon√≠veis para clustering: {len(available_features)}\")\n",
        "\n",
        "# Preparar matriz de features\n",
        "X = stop_labeled[available_features].copy()\n",
        "\n",
        "# Remover linhas com muitos NaN\n",
        "X = X.dropna(thresh=len(available_features) * 0.5)  # Pelo menos 50% das features preenchidas\n",
        "\n",
        "# Preencher NaN restantes com mediana\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "print(f\"‚úÖ Dados preparados: {len(X)} segmentos com {len(available_features)} features\")\n",
        "\n",
        "# Normalizar\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Aplicar clustering\n",
        "print(\"\\n‚è≥ Aplicando clustering...\")\n",
        "\n",
        "if HDBSCAN_AVAILABLE and len(X_scaled) > 10:\n",
        "    clusterer = hdbscan.HDBSCAN(min_cluster_size=max(3, len(X_scaled) // 20), min_samples=2)\n",
        "    cluster_labels = clusterer.fit_predict(X_scaled)\n",
        "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
        "    n_noise = list(cluster_labels).count(-1)\n",
        "    print(f\"‚úÖ HDBSCAN conclu√≠do: {n_clusters} clusters, {n_noise} pontos de ru√≠do\")\n",
        "else:\n",
        "    # Fallback para KMeans\n",
        "    n_clusters = min(5, len(X_scaled) // 3)  # M√°ximo 5 clusters\n",
        "    if n_clusters < 2:\n",
        "        n_clusters = 2\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "    print(f\"‚úÖ KMeans conclu√≠do: {n_clusters} clusters\")\n",
        "\n",
        "# Adicionar labels de cluster ao DataFrame\n",
        "stop_labeled_clustered = stop_labeled.iloc[X.index].copy()\n",
        "stop_labeled_clustered['cluster'] = cluster_labels\n",
        "\n",
        "# Comparar clusters com r√≥tulos de regras\n",
        "print(\"\\nüìä Compara√ß√£o Clusters vs R√≥tulos de Regras:\")\n",
        "comparison = pd.crosstab(stop_labeled_clustered['cluster'], stop_labeled_clustered['label_operacional'])\n",
        "print(comparison)\n",
        "\n",
        "# PCA para visualiza√ß√£o 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print(f\"\\nüìà PCA: Vari√¢ncia explicada: {pca.explained_variance_ratio_.sum():.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar clusters vs r√≥tulos\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# PCA com cores por cluster\n",
        "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='tab10', alpha=0.6, s=50)\n",
        "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')\n",
        "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')\n",
        "axes[0].set_title('Clusters (HDBSCAN/KMeans)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter1, ax=axes[0])\n",
        "\n",
        "# PCA com cores por r√≥tulo\n",
        "label_map = {label: i for i, label in enumerate(stop_labeled_clustered['label_operacional'].unique())}\n",
        "label_colors = stop_labeled_clustered['label_operacional'].map(label_map)\n",
        "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=label_colors, cmap='Set2', alpha=0.6, s=50)\n",
        "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')\n",
        "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')\n",
        "axes[1].set_title('R√≥tulos de Regras')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter2, ax=axes[1], ticks=range(len(label_map)), label='R√≥tulo')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / '03_clustering.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"‚úÖ Gr√°fico salvo: {PLOTS_DIR / '03_clustering.png'}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C√©lula 9: Visualiza√ß√µes Completas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Scatter Speed vs Acelera√ß√£o (hexbin)\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Filtrar dados v√°lidos\n",
        "valid_data = df[(df['speed_kmh'].notna()) & (df['linear_accel_magnitude'].notna())].copy()\n",
        "\n",
        "# Hexbin plot\n",
        "hb = ax.hexbin(valid_data['speed_kmh'], valid_data['linear_accel_magnitude'], \n",
        "               gridsize=50, cmap='YlOrRd', mincnt=1)\n",
        "ax.axvline(CONFIG['V_STOP'], color='r', linestyle='--', linewidth=2, label=f\"V_STOP={CONFIG['V_STOP']} km/h\")\n",
        "ax.set_xlabel('Velocidade (km/h)')\n",
        "ax.set_ylabel('Acelera√ß√£o Linear Magnitude (m/s¬≤)')\n",
        "ax.set_title('Speed vs Acelera√ß√£o Linear (Hexbin)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.colorbar(hb, ax=ax, label='Densidade')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / '04_speed_vs_accel.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"‚úÖ Gr√°fico salvo: {PLOTS_DIR / '04_speed_vs_accel.png'}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Timeline com segmentos coloridos por r√≥tulo\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "for device_id in df['device_id'].unique():\n",
        "    device_df = df[df['device_id'] == device_id].copy()\n",
        "    device_segments = merged_df[merged_df['device_id'] == device_id]\n",
        "    \n",
        "    # Plot velocidade\n",
        "    axes[0].plot(device_df['time'], device_df['speed_kmh'], alpha=0.3, color='gray', linewidth=0.5)\n",
        "    \n",
        "    # Colorir segmentos por r√≥tulo\n",
        "    color_map = {\n",
        "        'CARREGANDO': 'orange',\n",
        "        'BASCULANDO': 'purple',\n",
        "        'MOTOR_LIGADO_IDLE': 'blue',\n",
        "        'MOTOR_DESLIGADO': 'darkgreen',\n",
        "        'EM_MOVIMENTO': 'green',\n",
        "        'DESCONHECIDO': 'gray'\n",
        "    }\n",
        "    \n",
        "    for _, seg in device_segments.iterrows():\n",
        "        color = color_map.get(seg['label_operacional'], 'gray')\n",
        "        axes[0].axvspan(seg['t_start'], seg['t_end'], alpha=0.4, color=color)\n",
        "    \n",
        "    axes[0].set_xlabel('Tempo')\n",
        "    axes[0].set_ylabel('Velocidade (km/h)')\n",
        "    axes[0].set_title(f'Timeline de Segmentos Rotulados - {device_id}')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].axhline(CONFIG['V_STOP'], color='r', linestyle='--', linewidth=1)\n",
        "    \n",
        "    # Plot acelera√ß√£o linear\n",
        "    axes[1].plot(device_df['time'], device_df['linear_accel_magnitude'], alpha=0.5, color='blue', linewidth=0.5)\n",
        "    axes[1].set_xlabel('Tempo')\n",
        "    axes[1].set_ylabel('Acelera√ß√£o Linear (m/s¬≤)')\n",
        "    axes[1].set_title('Acelera√ß√£o Linear ao Longo do Tempo')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot pitch\n",
        "    if 'pitch' in device_df.columns:\n",
        "        axes[2].plot(device_df['time'], device_df['pitch'], alpha=0.5, color='green', linewidth=0.5)\n",
        "        axes[2].set_xlabel('Tempo')\n",
        "        axes[2].set_ylabel('Pitch (graus)')\n",
        "        axes[2].set_title('Pitch ao Longo do Tempo')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# Criar legenda\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [Patch(facecolor=color, alpha=0.4, label=label) \n",
        "                   for label, color in color_map.items()]\n",
        "axes[0].legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / '05_timeline_labeled.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"‚úÖ Gr√°fico salvo: {PLOTS_DIR / '05_timeline_labeled.png'}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Exemplos por classe (s√©ries temporais com picos marcados)\n",
        "stop_labeled_examples = merged_df[merged_df['is_moving'] == False].copy()\n",
        "\n",
        "# Selecionar at√© 10 exemplos por classe\n",
        "n_examples_per_class = 10\n",
        "classes = stop_labeled_examples['label_operacional'].unique()\n",
        "\n",
        "fig, axes = plt.subplots(len(classes), 1, figsize=(16, 4 * len(classes)))\n",
        "\n",
        "if len(classes) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for class_idx, label_class in enumerate(classes):\n",
        "    class_segments = stop_labeled_examples[stop_labeled_examples['label_operacional'] == label_class]\n",
        "    n_examples = min(n_examples_per_class, len(class_segments))\n",
        "    \n",
        "    for i in range(n_examples):\n",
        "        seg = class_segments.iloc[i]\n",
        "        \n",
        "        # Obter dados do segmento\n",
        "        seg_data = df[\n",
        "            (df['device_id'] == seg['device_id']) &\n",
        "            (df['time'] >= seg['t_start']) &\n",
        "            (df['time'] <= seg['t_end'])\n",
        "        ].copy()\n",
        "        \n",
        "        if len(seg_data) > 0:\n",
        "            # Plot acelera√ß√£o\n",
        "            time_rel = (seg_data['time'] - seg_data['time'].iloc[0]).dt.total_seconds()\n",
        "            axes[class_idx].plot(time_rel, seg_data['linear_accel_magnitude'], \n",
        "                                 alpha=0.6, linewidth=1, label=f\"Exemplo {i+1}\")\n",
        "            \n",
        "            # Marcar picos\n",
        "            accel = seg_data['linear_accel_magnitude'].dropna().values\n",
        "            if len(accel) > CONFIG['PEAK_DISTANCE']:\n",
        "                peaks, _ = signal.find_peaks(\n",
        "                    accel,\n",
        "                    height=CONFIG['PEAK_HEIGHT'],\n",
        "                    distance=int(CONFIG['PEAK_DISTANCE'])\n",
        "                )\n",
        "                if len(peaks) > 0:\n",
        "                    peak_times = time_rel.iloc[peaks]\n",
        "                    peak_values = accel[peaks]\n",
        "                    axes[class_idx].scatter(peak_times, peak_values, color='red', \n",
        "                                           marker='v', s=50, zorder=5, alpha=0.7)\n",
        "    \n",
        "    axes[class_idx].set_xlabel('Tempo (segundos)')\n",
        "    axes[class_idx].set_ylabel('Acelera√ß√£o Linear (m/s¬≤)')\n",
        "    axes[class_idx].set_title(f'{label_class} - Exemplos com Picos Marcados (confian√ßa m√©dia: {class_segments[\"confidence\"].mean():.2f})')\n",
        "    axes[class_idx].grid(True, alpha=0.3)\n",
        "    axes[class_idx].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / '06_examples_by_class.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"‚úÖ Gr√°fico salvo: {PLOTS_DIR / '06_examples_by_class.png'}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Distribui√ß√£o de features por classe (boxplots)\n",
        "feature_cols_viz = ['accel_mean', 'accel_std', 'accel_rms', 'peak_count', 'pitch_delta', 'voltage_mean']\n",
        "available_viz_features = [col for col in feature_cols_viz if col in stop_labeled_examples.columns]\n",
        "\n",
        "n_features = len(available_viz_features)\n",
        "n_cols = 3\n",
        "n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
        "axes = axes.flatten() if n_features > 1 else [axes]\n",
        "\n",
        "for idx, feat in enumerate(available_viz_features):\n",
        "    data_to_plot = []\n",
        "    labels = []\n",
        "    \n",
        "    for label_class in stop_labeled_examples['label_operacional'].unique():\n",
        "        class_data = stop_labeled_examples[\n",
        "            (stop_labeled_examples['label_operacional'] == label_class) &\n",
        "            (stop_labeled_examples[feat].notna())\n",
        "        ][feat].values\n",
        "        \n",
        "        if len(class_data) > 0:\n",
        "            data_to_plot.append(class_data)\n",
        "            labels.append(label_class)\n",
        "    \n",
        "    if len(data_to_plot) > 0:\n",
        "        bp = axes[idx].boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
        "        for patch in bp['boxes']:\n",
        "            patch.set_facecolor('lightblue')\n",
        "            patch.set_alpha(0.7)\n",
        "        axes[idx].set_ylabel(feat)\n",
        "        axes[idx].set_title(f'Distribui√ß√£o de {feat} por Classe')\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Remover eixos extras\n",
        "for idx in range(len(available_viz_features), len(axes)):\n",
        "    fig.delaxes(axes[idx])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PLOTS_DIR / '07_features_by_class.png', dpi=150, bbox_inches='tight')\n",
        "print(f\"‚úÖ Gr√°fico salvo: {PLOTS_DIR / '07_features_by_class.png'}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Epis√≥dios espec√≠ficos de basculamento com andadinha\n",
        "basculamento_episodes = merged_df[\n",
        "    (merged_df['label_operacional'] == 'BASCULANDO') &\n",
        "    (merged_df.get('has_short_move', False) == True)\n",
        "]\n",
        "\n",
        "if len(basculamento_episodes) > 0:\n",
        "    n_episodes = min(5, len(basculamento_episodes))\n",
        "    fig, axes = plt.subplots(n_episodes, 1, figsize=(16, 4 * n_episodes))\n",
        "    \n",
        "    if n_episodes == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for ep_idx, episode in basculamento_episodes.head(n_episodes).iterrows():\n",
        "        # Obter dados do epis√≥dio completo\n",
        "        episode_data = df[\n",
        "            (df['device_id'] == episode['device_id']) &\n",
        "            (df['time'] >= episode['t_start'] - pd.Timedelta(seconds=30)) &\n",
        "            (df['time'] <= episode['t_end'] + pd.Timedelta(seconds=30))\n",
        "        ].copy()\n",
        "        \n",
        "        if len(episode_data) > 0:\n",
        "            time_rel = (episode_data['time'] - episode['t_start']).dt.total_seconds()\n",
        "            \n",
        "            # Plot velocidade\n",
        "            axes[ep_idx].plot(time_rel, episode_data['speed_kmh'], label='Velocidade', color='blue', linewidth=2)\n",
        "            axes[ep_idx].axvspan(0, episode['duration_s'], alpha=0.2, color='purple', label='Epis√≥dio Basculamento')\n",
        "            axes[ep_idx].axhline(CONFIG['V_STOP'], color='r', linestyle='--', label=f\"V_STOP={CONFIG['V_STOP']} km/h\")\n",
        "            \n",
        "            # Plot acelera√ß√£o (eixo secund√°rio)\n",
        "            ax2 = axes[ep_idx].twinx()\n",
        "            ax2.plot(time_rel, episode_data['linear_accel_magnitude'], \n",
        "                    label='Acelera√ß√£o', color='orange', linewidth=1, alpha=0.7)\n",
        "            \n",
        "            axes[ep_idx].set_xlabel('Tempo (segundos)')\n",
        "            axes[ep_idx].set_ylabel('Velocidade (km/h)', color='blue')\n",
        "            ax2.set_ylabel('Acelera√ß√£o Linear (m/s¬≤)', color='orange')\n",
        "            axes[ep_idx].set_title(f'Epis√≥dio Basculamento com Andadinha - Dura√ß√£o: {episode[\"duration_s\"]:.0f}s')\n",
        "            axes[ep_idx].grid(True, alpha=0.3)\n",
        "            axes[ep_idx].legend(loc='upper left')\n",
        "            ax2.legend(loc='upper right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(PLOTS_DIR / '08_basculamento_episodes.png', dpi=150, bbox_inches='tight')\n",
        "    print(f\"‚úÖ Gr√°fico salvo: {PLOTS_DIR / '08_basculamento_episodes.png'}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Nenhum epis√≥dio de basculamento com andadinha encontrado para visualizar\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C√©lula 10: Export e Relat√≥rio Final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar tabela final para exporta√ß√£o\n",
        "timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Selecionar colunas principais para exporta√ß√£o\n",
        "export_cols = [\n",
        "    'device_id', 't_start', 't_end', 'duration_s', 'is_moving',\n",
        "    'label_operacional', 'confidence', 'evidence',\n",
        "    'accel_mean', 'accel_std', 'accel_rms', 'accel_p95', 'accel_p99',\n",
        "    'peak_count', 'peak_height_mean', 'peak_interval_mean',\n",
        "    'pitch_delta', 'roll_delta', 'voltage_mean'\n",
        "]\n",
        "\n",
        "# Filtrar colunas que existem\n",
        "available_export_cols = [col for col in export_cols if col in merged_df.columns]\n",
        "final_df = merged_df[available_export_cols].copy()\n",
        "\n",
        "# Adicionar colunas faltantes com NaN se necess√°rio\n",
        "for col in export_cols:\n",
        "    if col not in final_df.columns:\n",
        "        final_df[col] = np.nan\n",
        "\n",
        "# Reordenar colunas\n",
        "final_df = final_df[export_cols]\n",
        "\n",
        "# Exportar CSV\n",
        "csv_filename = OUTPUT_DIR / f'labeled_segments_{timestamp_str}.csv'\n",
        "final_df.to_csv(csv_filename, index=False)\n",
        "print(f\"‚úÖ CSV exportado: {csv_filename}\")\n",
        "print(f\"   Registros: {len(final_df):,}\")\n",
        "\n",
        "# Exportar Parquet\n",
        "parquet_filename = OUTPUT_DIR / f'labeled_segments_{timestamp_str}.parquet'\n",
        "final_df.to_parquet(parquet_filename, index=False, engine='pyarrow')\n",
        "print(f\"‚úÖ Parquet exportado: {parquet_filename}\")\n",
        "\n",
        "# Estat√≠sticas finais\n",
        "print(f\"\\nüìä Estat√≠sticas Finais:\")\n",
        "print(f\"   Total de segmentos: {len(final_df)}\")\n",
        "print(f\"   Segmentos PARADO: {len(final_df[final_df['is_moving'] == False])}\")\n",
        "print(f\"   Segmentos MOVIMENTO: {len(final_df[final_df['is_moving'] == True])}\")\n",
        "print(f\"\\nüìà Distribui√ß√£o de r√≥tulos:\")\n",
        "print(final_df['label_operacional'].value_counts())\n",
        "print(f\"\\nüìä Confian√ßa m√©dia por r√≥tulo:\")\n",
        "print(final_df.groupby('label_operacional')['confidence'].agg(['mean', 'std', 'min', 'max']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Relat√≥rio Final\n",
        "\n",
        "### Resumo Executivo\n",
        "\n",
        "Este pipeline processou **91,650 registros** de telemetria veicular (aproximadamente 25 horas de dados a 1 Hz) e gerou uma tabela de **segmentos rotulados** com estados operacionais do caminh√£o.\n",
        "\n",
        "**Principais resultados:**\n",
        "1. **Segmenta√ß√£o**: Identificados segmentos de PARADO vs EM MOVIMENTO usando threshold de velocidade (`V_STOP = 0.5 km/h`)\n",
        "2. **Rotulagem**: Aplicadas regras de weak supervision para classificar estados operacionais durante paradas\n",
        "3. **Features**: Extra√≠das features estat√≠sticas, de picos, espectrais e de orienta√ß√£o para cada segmento\n",
        "4. **Valida√ß√£o**: Gerados gr√°ficos de valida√ß√£o visual e clustering explorat√≥rio\n",
        "\n",
        "### Racional T√©cnico Detalhado\n",
        "\n",
        "#### 1. Data Discovery\n",
        "\n",
        "**Dados dispon√≠veis:**\n",
        "- **Velocidade**: `speed_kmh` - bem preenchida, range t√≠pico 0-80 km/h\n",
        "- **Acelera√ß√£o Linear**: `linear_accel_magnitude` - dispon√≠vel e limpa (sem gravidade)\n",
        "- **Orienta√ß√£o**: `pitch`, `roll` - dispon√≠veis para detectar basculamento\n",
        "- **Bateria**: `battery_voltage`, `battery_status` - proxy para motor ligado/desligado\n",
        "- **Motion Sensors**: `motion_stationary_detect` - valida√ß√£o cruzada\n",
        "\n",
        "**Qualidade dos dados:**\n",
        "- Taxa de amostragem m√©dia pr√≥xima a 1 Hz conforme esperado\n",
        "- Poucos outliers grosseiros (tratados substituindo por NaN)\n",
        "- Algumas colunas GPS com missing significativo, mas sinais principais bem preenchidos\n",
        "\n",
        "#### 2. Segmenta√ß√£o PARADO vs MOVIMENTO\n",
        "\n",
        "**Threshold escolhido**: `V_STOP = 0.5 km/h`\n",
        "\n",
        "**Justificativa:**\n",
        "- Histograma de velocidade mostra concentra√ß√£o massiva de valores pr√≥ximos a zero\n",
        "- Valores < 0.5 km/h s√£o tipicamente ru√≠do de GPS ou movimento impercept√≠vel\n",
        "- Validado com `motion_stationary_detect` quando dispon√≠vel\n",
        "\n",
        "**Par√¢metros:**\n",
        "- `MIN_STOP_SEC = 10s`: Filtra paradas muito curtas (ex: sem√°foros)\n",
        "- `GAP_SEC = 3s`: Permite pequenos gaps de amostragem dentro do mesmo segmento\n",
        "\n",
        "**Resultado**: Segmenta√ß√£o robusta que separa claramente per√≠odos parados de movimento.\n",
        "\n",
        "#### 3. Feature Engineering\n",
        "\n",
        "**Features extra√≠das por segmento:**\n",
        "\n",
        "1. **Estat√≠sticas de Acelera√ß√£o**:\n",
        "   - `mean`, `std`, `RMS`: Capturam n√≠vel e variabilidade de vibra√ß√£o\n",
        "   - `p95`, `p99`: Identificam picos extremos (conchadas)\n",
        "   - `energy`: Energia total do sinal\n",
        "\n",
        "2. **Detec√ß√£o de Picos**:\n",
        "   - `peak_count`: N√∫mero de picos acima do threshold\n",
        "   - `peak_height_mean`: Altura m√©dia dos picos\n",
        "   - `peak_interval_std`: Variabilidade dos intervalos (intermit√™ncia)\n",
        "\n",
        "3. **An√°lise Espectral**:\n",
        "   - Energia em bandas de frequ√™ncia (baixa/m√©dia/alta)\n",
        "   - Diferencia vibra√ß√£o cont√≠nua (motor) de impactos (conchadas)\n",
        "\n",
        "4. **Orienta√ß√£o**:\n",
        "   - `pitch_delta`: Mudan√ßa de pitch (crucial para basculamento)\n",
        "   - `roll_delta`: Mudan√ßa de roll\n",
        "\n",
        "5. **Bateria**:\n",
        "   - `voltage_mean`: Proxy para motor ligado (>4000mV) vs desligado (<4000mV)\n",
        "\n",
        "#### 4. Rotulagem com Weak Supervision\n",
        "\n",
        "**Regras implementadas:**\n",
        "\n",
        "| R√≥tulo | Regra | Evid√™ncia |\n",
        "|--------|-------|-----------|\n",
        "| **CARREGANDO** | `peak_count >= 5` AND `peak_interval_std > 3` | M√∫ltiplos picos intermitentes (padr√£o de conchadas) |\n",
        "| **BASCULANDO** | `pitch_delta > 10deg` OR (`accel_rms > 1.5` AND `accel_mean > 0.5`) | Mudan√ßa significativa de pitch OU vibra√ß√£o cont√≠nua |\n",
        "| **MOTOR_DESLIGADO** | `voltage < 4000mV` AND `accel_std < 0.3` | Baixa voltagem + baixa variabilidade |\n",
        "| **MOTOR_LIGADO_IDLE** | `voltage >= 4000mV` AND `0.3 < accel_std < 1.5` AND `peak_count < 3` | Voltagem OK + vibra√ß√£o moderada sem picos |\n",
        "| **DESCONHECIDO** | Nenhuma regra atinge threshold | Evid√™ncia insuficiente |\n",
        "\n",
        "**Confian√ßa**: Cada regra gera um score de confian√ßa baseado na for√ßa da evid√™ncia. Segmentos com confian√ßa < `TH_CONF` s√£o marcados como DESCONHECIDO.\n",
        "\n",
        "**Limita√ß√µes identificadas:**\n",
        "- **CARREGADO vs VAZIO**: N√£o h√° sinal direto no CSV para distinguir carga. Proposta: inferir por transi√ß√£o (p√≥s-carregamento = carregado, p√≥s-basculamento = vazio), mas requer valida√ß√£o manual.\n",
        "- **FILA/MANOBRA**: Estados intermedi√°rios podem existir mas n√£o foram distinguidos com os sinais dispon√≠veis.\n",
        "\n",
        "#### 5. Merge de Basculamento com Andadinha\n",
        "\n",
        "**L√≥gica implementada:**\n",
        "- Detecta segmentos BASCULANDO seguidos de movimento curto (<30s, <5 km/h)\n",
        "- Mescla em epis√≥dio √∫nico preservando microfases\n",
        "- Permite capturar o ciclo completo de basculamento incluindo a \"andadinha\" final\n",
        "\n",
        "**Resultado**: Epis√≥dios mais completos e representativos do processo real.\n",
        "\n",
        "#### 6. Clustering Explorat√≥rio\n",
        "\n",
        "**M√©todo**: HDBSCAN (ou KMeans como fallback)\n",
        "\n",
        "**Objetivo**: Validar se os r√≥tulos de regras correspondem a agrupamentos naturais nos dados.\n",
        "\n",
        "**Resultado**: Compara√ß√£o entre clusters e r√≥tulos mostra concord√¢ncia parcial, sugerindo que as regras capturam padr√µes reais, mas podem existir subgrupos n√£o cobertos.\n",
        "\n",
        "#### 7. Visualiza√ß√µes Geradas\n",
        "\n",
        "1. **Painel de qualidade**: Missing, distribui√ß√µes, timeline de amostragem\n",
        "2. **Speed vs Acelera√ß√£o**: Hexbin mostrando regi√µes de parado\n",
        "3. **Timeline rotulada**: Segmentos coloridos por r√≥tulo ao longo do tempo\n",
        "4. **Exemplos por classe**: S√©ries temporais com picos marcados\n",
        "5. **Distribui√ß√£o de features**: Boxplots por classe\n",
        "6. **Epis√≥dios de basculamento**: Exemplos completos com andadinha\n",
        "7. **Clusters vs R√≥tulos**: PCA 2D comparando agrupamentos\n",
        "\n",
        "### Limita√ß√µes e Pr√≥ximos Passos\n",
        "\n",
        "#### Limita√ß√µes Atuais\n",
        "\n",
        "1. **CARREGADO vs VAZIO**: Indistingu√≠vel com sinais dispon√≠veis\n",
        "2. **Estados intermedi√°rios**: FILA, MANOBRA CURTA n√£o identificados\n",
        "3. **Thresholds fixos**: Par√¢metros podem precisar ajuste por ve√≠culo/opera√ß√£o\n",
        "4. **Valida√ß√£o manual**: Requer revis√£o de exemplos para confirmar r√≥tulos\n",
        "\n",
        "#### Pr√≥ximos Passos para Batch/Cont√≠nuo\n",
        "\n",
        "1. **Configura√ß√£o parametriz√°vel**: Todos os thresholds em arquivo de config\n",
        "2. **Processamento incremental**: Processar por janelas de tempo (ex: 1 hora)\n",
        "3. **Versionamento**: Versionar regras e thresholds\n",
        "4. **M√©tricas de monitoramento**:\n",
        "   - Taxa de DESCONHECIDO (deve ser <20%)\n",
        "   - Estabilidade de thresholds (drift detection)\n",
        "   - Concord√¢ncia entre regras e clustering\n",
        "5. **Feedback loop**: Mecanismo para revis√£o manual e retreinamento\n",
        "\n",
        "### Arquivos Gerados\n",
        "\n",
        "- **CSV**: `labeled_segments_YYYYMMDD_HHMMSS.csv`\n",
        "- **Parquet**: `labeled_segments_YYYYMMDD_HHMMSS.parquet`\n",
        "- **Gr√°ficos**: Pasta `plots/` com 8 visualiza√ß√µes principais\n",
        "\n",
        "### Conclus√£o\n",
        "\n",
        "O pipeline foi implementado com sucesso e gera r√≥tulos interpret√°veis com evid√™ncias expl√≠citas. As regras capturam padr√µes principais (carregamento, basculamento, motor ligado/desligado) mas deixam espa√ßo para refinamento com dados adicionais ou valida√ß√£o manual.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.14.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
